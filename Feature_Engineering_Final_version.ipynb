{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature_Engineering_Final_version\n",
    "## IEEE Fraud Detection (Kaggle Current Competition)\n",
    "\n",
    "\n",
    "Training Part:\\\n",
    "train_transaction\\\n",
    "train_identity\n",
    "\n",
    "\n",
    "Testing Part:\\\n",
    "test_transaction\\\n",
    "test_identity\n",
    "\n",
    "Dataset can be downloaded via:\\\n",
    "https://www.kaggle.com/c/ieee-fraud-detection/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " # import basic package\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import style\n",
    "\n",
    "#Deal with warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "style.use('ggplot')\n",
    "color_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n",
    "\n",
    "#import my own packages\n",
    "# import fraud_fun\n",
    "# import myfunc_pd as mf\n",
    "\n",
    "\n",
    "# from package import binary encoder\n",
    "import category_encoders as ce\n",
    "\n",
    "#import package for ML and feature engineering\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the trian and test dataset.\n",
    "train_transaction = pd.read_csv('train_transaction.csv')\n",
    "train_identity = pd.read_csv('train_identity.csv')\n",
    "test_transaction = pd.read_csv('test_transaction.csv')\n",
    "test_identity = pd.read_csv('test_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two dataset, \n",
    "\n",
    "train = train_transaction.merge(train_identity, how='left', on='TransactionID')\n",
    "# train.head()\n",
    "test = test_transaction.merge(test_identity, how='left', on='TransactionID')\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>...</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2987000</td>\n",
       "      <td>0</td>\n",
       "      <td>86400</td>\n",
       "      <td>68.5</td>\n",
       "      <td>W</td>\n",
       "      <td>13926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>discover</td>\n",
       "      <td>142.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2987001</td>\n",
       "      <td>0</td>\n",
       "      <td>86401</td>\n",
       "      <td>29.0</td>\n",
       "      <td>W</td>\n",
       "      <td>2755</td>\n",
       "      <td>404.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2987002</td>\n",
       "      <td>0</td>\n",
       "      <td>86469</td>\n",
       "      <td>59.0</td>\n",
       "      <td>W</td>\n",
       "      <td>4663</td>\n",
       "      <td>490.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2987003</td>\n",
       "      <td>0</td>\n",
       "      <td>86499</td>\n",
       "      <td>50.0</td>\n",
       "      <td>W</td>\n",
       "      <td>18132</td>\n",
       "      <td>567.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2987004</td>\n",
       "      <td>0</td>\n",
       "      <td>86506</td>\n",
       "      <td>50.0</td>\n",
       "      <td>H</td>\n",
       "      <td>4497</td>\n",
       "      <td>514.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>samsung browser 6.2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2220x1080</td>\n",
       "      <td>match_status:2</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>mobile</td>\n",
       "      <td>SAMSUNG SM-G892A Build/NRD90M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  isFraud  TransactionDT  TransactionAmt ProductCD  card1  \\\n",
       "0        2987000        0          86400            68.5         W  13926   \n",
       "1        2987001        0          86401            29.0         W   2755   \n",
       "2        2987002        0          86469            59.0         W   4663   \n",
       "3        2987003        0          86499            50.0         W  18132   \n",
       "4        2987004        0          86506            50.0         H   4497   \n",
       "\n",
       "   card2  card3       card4  card5  ...                id_31  id_32  \\\n",
       "0    NaN  150.0    discover  142.0  ...                  NaN    NaN   \n",
       "1  404.0  150.0  mastercard  102.0  ...                  NaN    NaN   \n",
       "2  490.0  150.0        visa  166.0  ...                  NaN    NaN   \n",
       "3  567.0  150.0  mastercard  117.0  ...                  NaN    NaN   \n",
       "4  514.0  150.0  mastercard  102.0  ...  samsung browser 6.2   32.0   \n",
       "\n",
       "       id_33           id_34  id_35 id_36 id_37  id_38  DeviceType  \\\n",
       "0        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "1        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "2        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "3        NaN             NaN    NaN   NaN   NaN    NaN         NaN   \n",
       "4  2220x1080  match_status:2      T     F     T      T      mobile   \n",
       "\n",
       "                      DeviceInfo  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4  SAMSUNG SM-G892A Build/NRD90M  \n",
       "\n",
       "[5 rows x 434 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>ProductCD</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card4</th>\n",
       "      <th>card5</th>\n",
       "      <th>card6</th>\n",
       "      <th>...</th>\n",
       "      <th>id_31</th>\n",
       "      <th>id_32</th>\n",
       "      <th>id_33</th>\n",
       "      <th>id_34</th>\n",
       "      <th>id_35</th>\n",
       "      <th>id_36</th>\n",
       "      <th>id_37</th>\n",
       "      <th>id_38</th>\n",
       "      <th>DeviceType</th>\n",
       "      <th>DeviceInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>18403224</td>\n",
       "      <td>31.95</td>\n",
       "      <td>W</td>\n",
       "      <td>10409</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>18403263</td>\n",
       "      <td>49.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4272</td>\n",
       "      <td>111.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>18403310</td>\n",
       "      <td>171.00</td>\n",
       "      <td>W</td>\n",
       "      <td>4476</td>\n",
       "      <td>574.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>226.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>18403310</td>\n",
       "      <td>284.95</td>\n",
       "      <td>W</td>\n",
       "      <td>10989</td>\n",
       "      <td>360.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>visa</td>\n",
       "      <td>166.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>18403317</td>\n",
       "      <td>67.95</td>\n",
       "      <td>W</td>\n",
       "      <td>18018</td>\n",
       "      <td>452.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>mastercard</td>\n",
       "      <td>117.0</td>\n",
       "      <td>debit</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 433 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TransactionID  TransactionDT  TransactionAmt ProductCD  card1  card2  \\\n",
       "0        3663549       18403224           31.95         W  10409  111.0   \n",
       "1        3663550       18403263           49.00         W   4272  111.0   \n",
       "2        3663551       18403310          171.00         W   4476  574.0   \n",
       "3        3663552       18403310          284.95         W  10989  360.0   \n",
       "4        3663553       18403317           67.95         W  18018  452.0   \n",
       "\n",
       "   card3       card4  card5  card6  ...  id_31  id_32  id_33  id_34 id_35  \\\n",
       "0  150.0        visa  226.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
       "1  150.0        visa  226.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
       "2  150.0        visa  226.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
       "3  150.0        visa  166.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
       "4  150.0  mastercard  117.0  debit  ...    NaN    NaN    NaN    NaN   NaN   \n",
       "\n",
       "  id_36  id_37  id_38  DeviceType  DeviceInfo  \n",
       "0   NaN    NaN    NaN         NaN         NaN  \n",
       "1   NaN    NaN    NaN         NaN         NaN  \n",
       "2   NaN    NaN    NaN         NaN         NaN  \n",
       "3   NaN    NaN    NaN         NaN         NaN  \n",
       "4   NaN    NaN    NaN         NaN         NaN  \n",
       "\n",
       "[5 rows x 433 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** finish detection **********\n",
      "train_missing_cat has 31 features\n",
      "train_missing_num has 403 features\n",
      "test_missing_cat has 31 features\n",
      "test_missing_num has 402 features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_missing_cat = []\n",
    "train_missing_num = []\n",
    "test_missing_cat = []\n",
    "test_missing_num = []\n",
    "for col in train.columns:\n",
    "    \n",
    "    if train[col].dtypes == 'object':\n",
    "        train_missing_cat.append(col)     \n",
    "    else:\n",
    "        train_missing_num.append(col)\n",
    "        \n",
    "for col_ in test.columns:\n",
    "    \n",
    "    if test[col_].dtypes == 'object':\n",
    "        test_missing_cat.append(col_)     \n",
    "    else:\n",
    "        test_missing_num.append(col_)        \n",
    "\n",
    "print(f'****** finish detection **********')\n",
    "print(f'train_missing_cat has {len(train_missing_cat)} features')\n",
    "print(f'train_missing_num has {len(train_missing_num)} features')\n",
    "print(f'test_missing_cat has {len(test_missing_cat)} features')\n",
    "print(f'test_missing_num has {len(test_missing_num)} features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_missing_num:\n",
    "    train[i].fillna((train[i].mean()), inplace=True)\n",
    "for i in train_missing_cat:\n",
    "    train[i].fillna('NotFound', inplace=True)\n",
    "    \n",
    "for i in test_missing_num:\n",
    "    test[i].fillna((test[i].mean()), inplace=True)\n",
    "for i in test_missing_cat:\n",
    "    test[i].fillna('NotFound', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns='TransactionID',inplace=True)\n",
    "test.drop(columns='TransactionID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 433), (506691, 432))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In this section, we will maily focus on TransactionDT column, to generate some potential patterns.\n",
    "\n",
    "Several randomly mean and std will be generated according to the feature in same sub_sections (V1-V399 etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2017-12-02 00:00:00\n",
      "1   2017-12-02 00:00:01\n",
      "2   2017-12-02 00:01:09\n",
      "3   2017-12-02 00:01:39\n",
      "4   2017-12-02 00:01:46\n",
      "5   2017-12-02 00:01:50\n",
      "6   2017-12-02 00:02:02\n",
      "7   2017-12-02 00:02:09\n",
      "8   2017-12-02 00:02:15\n",
      "9   2017-12-02 00:02:16\n",
      "Name: TransactionDT, dtype: datetime64[ns]\n",
      "0   2018-07-02 00:00:24\n",
      "1   2018-07-02 00:01:03\n",
      "2   2018-07-02 00:01:50\n",
      "3   2018-07-02 00:01:50\n",
      "4   2018-07-02 00:01:57\n",
      "5   2018-07-02 00:02:03\n",
      "6   2018-07-02 00:02:30\n",
      "7   2018-07-02 00:03:07\n",
      "8   2018-07-02 00:03:25\n",
      "9   2018-07-02 00:03:36\n",
      "Name: TransactionDT, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Processing date from reference:\n",
    "#https://www.kaggle.com/kevinbonnes/transactiondt-starting-at-2017-12-01\n",
    "\n",
    "\n",
    "START_DATE = '2017-12-01'\n",
    "# df = pd.concat([train_transaction, test_transaction], axis = 0, sort = False)\n",
    "# df = train_transaction.merge(train_identity, how='left', on='TransactionID')\n",
    "# Preprocess date column\n",
    "startdate = datetime.datetime.strptime(START_DATE, '%Y-%m-%d')\n",
    "train['TransactionDT'] = train['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "test['TransactionDT'] = test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds = x)))\n",
    "print(train['TransactionDT'].head(10))\n",
    "print(test['TransactionDT'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 436), (506691, 435))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the different aspect into the new columns according to the date format.\n",
    "# df['Year'] = pd.to_datetime(df['TransactionDT']).dt.year\n",
    "train['Month'] = pd.to_datetime(train['TransactionDT']).dt.month\n",
    "train['Day'] = pd.to_datetime(train['TransactionDT']).dt.day\n",
    "train['Hour'] = pd.to_datetime(train['TransactionDT']).dt.hour\n",
    "train['Dayofweek'] = pd.to_datetime(train['TransactionDT']).dt.dayofweek\n",
    "\n",
    "test['Month'] = pd.to_datetime(test['TransactionDT']).dt.month\n",
    "test['Day'] = pd.to_datetime(test['TransactionDT']).dt.day\n",
    "test['Hour'] = pd.to_datetime(test['TransactionDT']).dt.hour\n",
    "test['Dayofweek'] = pd.to_datetime(test['TransactionDT']).dt.dayofweek\n",
    "# put the TranscationDT column into drop_columns list\n",
    "\n",
    "train.drop(columns='TransactionDT',inplace=True)\n",
    "test.drop(columns='TransactionDT',inplace=True)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the different numerical sub_features such as v_feature, id_feature\n",
    "\n",
    "card_features = ['card1','card2','card3','card5']\n",
    "c_features = [c for c in train.columns if 'C' in c and c != 'ProductCD' and c not in \\\n",
    "             train_missing_cat]\n",
    "d_features = [d for d in train.columns if 'D' in d and (d != 'TransactionID')\\\n",
    "        and (d != 'TransactionDT') and (d != 'ProductCD')and (d != 'DeviceType')and\\\n",
    "         (d != 'DeviceInfo') and (d not in train_missing_cat)\\\n",
    "             and (d != 'Day') and (d != 'Dayofweek')]\n",
    "v_features = [c for c in train.columns if c[0] == 'V' and c not in \\\n",
    "             train_missing_cat]\n",
    "id_features = [c for c in train.columns if c[0:2] == 'id' and c not in \\\n",
    "             train_missing_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 446), (506691, 445))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Apply the mean and std for each sub_features \n",
    "train['card_features_mean'] = train[card_features].mean(axis=1)\n",
    "train['card_features_std'] = train[card_features].std(axis=1)\n",
    "train['c_features_mean'] = train[c_features].mean(axis=1)\n",
    "train['c_features_std'] = train[c_features].std(axis=1)\n",
    "train['d_features_mean'] = train[d_features].mean(axis=1)\n",
    "train['d_features_std'] = train[d_features].std(axis=1)\n",
    "train['v_features_mean'] = train[v_features].mean(axis=1)\n",
    "train['v_features_std'] = train[v_features].std(axis=1)\n",
    "train['id_features_mean'] = train[id_features].mean(axis=1)\n",
    "train['id_features_std'] = train[id_features].std(axis=1)\n",
    "\n",
    "## Apply the mean and std for each sub_features \n",
    "test['card_features_mean'] = test[card_features].mean(axis=1)\n",
    "test['card_features_std'] = test[card_features].std(axis=1)\n",
    "test['c_features_mean'] = test[c_features].mean(axis=1)\n",
    "test['c_features_std'] = test[c_features].std(axis=1)\n",
    "test['d_features_mean'] = test[d_features].mean(axis=1)\n",
    "test['d_features_std'] = test[d_features].std(axis=1)\n",
    "test['v_features_mean'] = test[v_features].mean(axis=1)\n",
    "test['v_features_std'] = test[v_features].std(axis=1)\n",
    "test['id_features_mean'] = test[id_features].mean(axis=1)\n",
    "test['id_features_std'] = test[id_features].std(axis=1)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 454), (506691, 453))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try to use card features\n",
    "for col in ['card1','card2','card4','card5','card6']:\n",
    "    # we are just taking a mean for each group and diving it with the each group Transaction amount to get more information\n",
    "    # and also std for each group \n",
    "    train['Transactionamt_mean_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('mean'))\n",
    "    train['Transactionamt_std_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('std'))\n",
    "    test['Transactionamt_mean_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('mean'))\n",
    "    test['Transactionamt_std_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('std'))\n",
    "\n",
    "# Drop the features which will cause errors for future Normalisation \n",
    "train.drop(columns=['Transactionamt_std_card1','Transactionamt_std_card5'],inplace=True)\n",
    "test.drop(columns=['Transactionamt_std_card1','Transactionamt_std_card5'],inplace=True)\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Transactionamt_std_card2'].fillna(method='ffill',inplace=True)\n",
    "test['Transactionamt_std_card6'].fillna(method='ffill',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 454), (506691, 453))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature engineering Id_cols \n",
    "# Part from Reference :https://www.kaggle.com/kabure/almost-complete-feature-engineering-ieee-data\n",
    "# Set-up new column to get the info from Deviceinfo, add label for them./\n",
    "\n",
    "train.loc[train['DeviceInfo'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "train.loc[train['DeviceInfo'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "train.loc[train['DeviceInfo'].str.contains('GT-', na=False), 'device_name'] = 'Samsung' \n",
    "train.loc[train['DeviceInfo'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "train.loc[train['DeviceInfo'].str.contains('Moto', na=False), 'device_name'] = 'Motorola' \n",
    "train.loc[train['DeviceInfo'].str.contains('moto', na=False), 'device_name'] = 'Motorola' \n",
    "train.loc[train['DeviceInfo'].str.contains('LG-', na=False), 'device_name'] = 'LG' \n",
    "train.loc[train['DeviceInfo'].str.contains('rv:', na=False), 'device_name'] = 'RV' \n",
    "train.loc[train['DeviceInfo'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei' \n",
    "train.loc[train['DeviceInfo'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei' \n",
    "train.loc[train['DeviceInfo'].str.contains('-L', na=False), 'device_name'] = 'Huawei' \n",
    "train.loc[train['DeviceInfo'].str.contains('Blade', na=False), 'device_name'] = 'ZTE' \n",
    "train.loc[train['DeviceInfo'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE' \n",
    "train.loc[train['DeviceInfo'].str.contains('Linux', na=False), 'device_name'] = 'Linux' \n",
    "train.loc[train['DeviceInfo'].str.contains('XT', na=False), 'device_name'] = 'Sony' \n",
    "train.loc[train['DeviceInfo'].str.contains('HTC', na=False), 'device_name'] = 'HTC' \n",
    "train.loc[train['DeviceInfo'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "train.loc[train['DeviceInfo'].str.contains('Not', na=True), 'device_name'] = 'NotFound'\n",
    "\n",
    "test.loc[test['DeviceInfo'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\n",
    "test.loc[test['DeviceInfo'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\n",
    "test.loc[test['DeviceInfo'].str.contains('GT-', na=False), 'device_name'] = 'Samsung' \n",
    "test.loc[test['DeviceInfo'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\n",
    "test.loc[test['DeviceInfo'].str.contains('Moto', na=False), 'device_name'] = 'Motorola' \n",
    "test.loc[test['DeviceInfo'].str.contains('moto', na=False), 'device_name'] = 'Motorola' \n",
    "test.loc[test['DeviceInfo'].str.contains('LG-', na=False), 'device_name'] = 'LG' \n",
    "test.loc[test['DeviceInfo'].str.contains('rv:', na=False), 'device_name'] = 'RV' \n",
    "test.loc[test['DeviceInfo'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei' \n",
    "test.loc[test['DeviceInfo'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei' \n",
    "test.loc[test['DeviceInfo'].str.contains('-L', na=False), 'device_name'] = 'Huawei' \n",
    "test.loc[test['DeviceInfo'].str.contains('Blade', na=False), 'device_name'] = 'ZTE' \n",
    "test.loc[test['DeviceInfo'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE' \n",
    "test.loc[test['DeviceInfo'].str.contains('Linux', na=False), 'device_name'] = 'Linux' \n",
    "test.loc[test['DeviceInfo'].str.contains('XT', na=False), 'device_name'] = 'Sony' \n",
    "test.loc[test['DeviceInfo'].str.contains('HTC', na=False), 'device_name'] = 'HTC' \n",
    "test.loc[test['DeviceInfo'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\n",
    "test.loc[test['DeviceInfo'].str.contains('Not', na=True), 'device_name'] = 'NotFound'\n",
    "# Drop the previous column.\n",
    "train.drop(columns='DeviceInfo',inplace=True)\n",
    "test.drop(columns='DeviceInfo',inplace=True)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['device_name'].fillna('NotFound',inplace=True)\n",
    "train['device_name'].fillna('NotFound',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n",
    "us_emails = ['gmail', 'net', 'edu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature Enginnering based on ProductCD because it has only 4 levels\n",
    "# and also for P_emaildomain ,R_emaildomain,DeviceType\n",
    "#DeviceInfo,id_15,id_23,id_30,id_31,id_34\n",
    "\n",
    "for col in ['ProductCD', 'P_emaildomain','R_emaildomain','DeviceType','device_name','id_15','id_23','id_30','id_34']:\n",
    "    train['Transactionamt_mean_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('mean'))\n",
    "    train['Transactionamt_std_'+str(col)]=(train['TransactionAmt']/train.groupby(col)['TransactionAmt'].transform('std'))\n",
    "    \n",
    "    test['Transactionamt_mean_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('mean'))\n",
    "    test['Transactionamt_std_'+str(col)]=(test['TransactionAmt']/test.groupby(col)['TransactionAmt'].transform('std'))\n",
    "# Part Reference:  https://www.kaggle.com/davidcairuz/feature-engineering-lightgbm-corrected\n",
    "\n",
    "# feature engineering with TransactionAmt\n",
    "\n",
    "train['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "train['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "#feature engineering with id_02\n",
    "train['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\n",
    "train['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n",
    "#feature engineering with D_15\n",
    "train['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\n",
    "train['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n",
    "train['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\n",
    "train['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\n",
    "train['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n",
    "\n",
    "test['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\n",
    "test['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n",
    "#feature engineering with id_02\n",
    "test['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\n",
    "test['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n",
    "#feature engineering with D_15\n",
    "test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n",
    "test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n",
    "test['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\n",
    "test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n",
    "test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n",
    "#Reference: https://www.kaggle.com/c/ieee-fraud-detection/discussion/100499\n",
    "\n",
    "for c in ['P_emaildomain', 'R_emaildomain']:\n",
    "    train[c + '_bin'] = train[c].map(emails)\n",
    "    test[c + '_bin'] = test[c].map(emails)\n",
    "    \n",
    "    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n",
    "    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n",
    "    \n",
    "    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n",
    "    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['D15_to_mean_addr1'].fillna(train['D15_to_mean_addr1'].mean(),inplace=True)\n",
    "train['Transactionamt_std_id_30'].fillna(train['Transactionamt_std_id_30'].mean(),inplace=True)\n",
    "train['P_emaildomain_bin'].fillna('NotFound',inplace=True)\n",
    "train['R_emaildomain_bin'].fillna('NotFound',inplace=True)\n",
    "\n",
    "\n",
    "test['D15_to_mean_addr1'].fillna(test['D15_to_mean_addr1'].mean(),inplace=True)\n",
    "test['Transactionamt_std_id_34'].fillna(test['Transactionamt_std_id_34'].mean(),inplace=True)\n",
    "test['P_emaildomain_bin'].fillna('NotFound',inplace=True)\n",
    "test['R_emaildomain_bin'].fillna('NotFound',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New feature - log of transaction amount. ()\n",
    "train['TransactionAmt_Log'] = np.log(train['TransactionAmt'])\n",
    "test['TransactionAmt_Log'] = np.log(test['TransactionAmt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 483), (506691, 482))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the features which will cause errors for future Normalisation \n",
    "train.drop(columns=['Transactionamt_std_P_emaildomain'],inplace=True)\n",
    "test.drop(columns=['Transactionamt_std_P_emaildomain'],inplace=True)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bakcup\n",
    "# train.to_csv('FullData_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bakcup\n",
    "# test.to_csv('FullData_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('FullData_train.csv')\n",
    "# train.drop(columns='Unnamed: 0',inplace=True)\n",
    "# # train.head()\n",
    "# test = pd.read_csv('FullData_test.csv')\n",
    "# test.drop(columns='Unnamed: 0',inplace=True)\n",
    "# # train.head()\n",
    "\n",
    "# train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** finish detection **********\n",
      "train_missing_cat has 35 features\n",
      "train_missing_num has 448 features\n",
      "test_missing_cat has 35 features\n",
      "test_missing_num has 447 features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_missing_cat = []\n",
    "train_missing_num = []\n",
    "test_missing_cat = []\n",
    "test_missing_num = []\n",
    "for col in train.columns:\n",
    "    \n",
    "    if train[col].dtypes == 'object':\n",
    "        train_missing_cat.append(col)     \n",
    "    else:\n",
    "        train_missing_num.append(col)\n",
    "        \n",
    "for col_ in test.columns:\n",
    "    \n",
    "    if test[col_].dtypes == 'object':\n",
    "        test_missing_cat.append(col_)     \n",
    "    else:\n",
    "        test_missing_num.append(col_)        \n",
    "print(f'****** finish detection **********')\n",
    "print(f'train_missing_cat has {len(train_missing_cat)} features')\n",
    "print(f'train_missing_num has {len(train_missing_num)} features')\n",
    "print(f'test_missing_cat has {len(test_missing_cat)} features')\n",
    "print(f'test_missing_num has {len(test_missing_num)} features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large_col_cat_train: ['P_emaildomain', 'R_emaildomain', 'id_30', 'id_31', 'id_33', 'device_name', 'P_emaildomain_bin', 'P_emaildomain_suffix', 'R_emaildomain_bin', 'R_emaildomain_suffix']\n",
      "small_col_cat_train: ['ProductCD', 'card4', 'card6', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType']\n"
     ]
    }
   ],
   "source": [
    "# train_missing_cat.remove('DeviceInfo')\n",
    "large_col_cat_train = []\n",
    "small_col_cat_train = []\n",
    "\n",
    "for feature in train_missing_cat:\n",
    "    if train[feature].nunique() > 5:\n",
    "        large_col_cat_train.append(feature)\n",
    "    else: \n",
    "        small_col_cat_train.append(feature)\n",
    "print(f'large_col_cat_train: {large_col_cat_train}')\n",
    "print(f'small_col_cat_train: {small_col_cat_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "large_col_cat_test: ['P_emaildomain', 'R_emaildomain', 'id_30', 'id_31', 'id_33', 'device_name', 'P_emaildomain_bin', 'P_emaildomain_suffix', 'R_emaildomain_bin', 'R_emaildomain_suffix']\n",
      "small_col_cat_test: ['ProductCD', 'card4', 'card6', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType']\n"
     ]
    }
   ],
   "source": [
    "# test_missing_cat.remove('DeviceInfo')\n",
    "large_col_cat_test = []\n",
    "small_col_cat_test = []\n",
    "\n",
    "for feature in test_missing_cat:\n",
    "    if test[feature].nunique() > 5:\n",
    "        large_col_cat_test.append(feature)\n",
    "    else: \n",
    "        small_col_cat_test.append(feature)\n",
    "print(f'large_col_cat_test: {large_col_cat_test}')\n",
    "print(f'small_col_cat_test: {small_col_cat_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For large_col_cat, the binary labelencoder will be applied.\n",
    "df_binary = binary_encoder(train,large_col_cat_train)\n",
    "train = pd.concat([train,df_binary],axis=1)\n",
    "\n",
    "\n",
    "#For small_col_cat, the get_dummies will be applied.\n",
    "df_dummies = get_dummies_label(train,small_col_cat_train)\n",
    "train = pd.concat([train,df_dummies],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 606), (506691, 602))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For large_col_cat, the binary labelencoder will be applied.\n",
    "df_binary = binary_encoder(test,large_col_cat_test)\n",
    "test = pd.concat([test,df_binary],axis=1)\n",
    "\n",
    "\n",
    "#For small_col_cat, the get_dummies will be applied.\n",
    "df_dummies = get_dummies_label(test,small_col_cat_test)\n",
    "test = pd.concat([test,df_dummies],axis=1)\n",
    "\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 571), (506691, 567))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop(columns=large_col_cat_train, inplace=True)\n",
    "train.drop(columns=small_col_cat_train, inplace=True)\n",
    "\n",
    "test.drop(columns=large_col_cat_test, inplace=True)\n",
    "test.drop(columns=small_col_cat_test, inplace=True)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = train.values\n",
    "# create scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform in one step\n",
    "df2 = scaler.fit_transform(array)\n",
    "\n",
    "train = pd.DataFrame(df2, columns=train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 571), (506691, 567))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_ = test.values\n",
    "# create scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform in one step\n",
    "df_test = scaler.fit_transform(array_)\n",
    "\n",
    "test = pd.DataFrame(df_test, columns=test.columns)\n",
    "\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bakcup\n",
    "train.to_csv('FullData_nor_train.csv')\n",
    "test.to_csv('FullData_nor_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cols = [c for c in train.columns if c not in test.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['isFraud',\n",
       " 'card6_debit or credit',\n",
       " 'id_34_match_status:-1',\n",
       " 'id_34_match_status:0']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['card6_debit or credit'] = 0\n",
    "test['id_34_match_status:-1'] = 0\n",
    "test['id_34_match_status:0'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('FullData_nor_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isna().any().sum(), test.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((590540, 571), (506691, 570))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_encoder(DataFrame, col):\n",
    "\n",
    "    # Using the binary encoder to deal with large_col_cat:\n",
    "\n",
    "    df = DataFrame[col]\n",
    "\n",
    "    # Calling encoder function.\n",
    "    encoder = ce.BinaryEncoder(cols=col)\n",
    "    df_binary = encoder.fit_transform(df)\n",
    "    return df_binary\n",
    "    # Concat the features with previous dataset\n",
    "#     DataFrame = pd.concat([DataFrame,df_binary],axis=1)\n",
    "#     DataFrame.drop(columns=col, inplace=True)\n",
    "    print(f'************** Finish binary_encoder *****************')\n",
    "    \n",
    "    \n",
    "\n",
    "def get_dummies_label(DataFrame, col):\n",
    "    # Using dummies to deal with the small amount of unique values in small_col_cat\n",
    "    df_small = DataFrame[col]\n",
    "    # Get dummies\n",
    "    df_dummies = pd.get_dummies(df_small, prefix_sep='_', drop_first=True)\n",
    "    return df_dummies\n",
    "    # Concat the features with previous dataset\n",
    "#     DataFrame = pd.concat([DataFrame,df_dummies],axis=1)\n",
    "#     DataFrame.drop(columns=col, inplace=True)\n",
    "    \n",
    "    print(f'************** Finish get_dummies_label *****************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
